#!/bin/bash
#SBATCH --job-name=SLaK_tiny_sequential
#SBATCH -p gpu
#SBATCH -N 1
#SBATCH --ntasks-per-node=4
#SBATCH --gpus-per-node=4
#SBATCH --gpus=4
#SBATCH -t 1-23:59:59
#SBATCH --exclusive
#SBATCH --cpus-per-task=18
#SBATCH -o SLaK_tiny_sequential.out

source /home/sliu/miniconda3/etc/profile.d/conda.sh
source activate slak



model=SLAK_Attention_tiny
python -m torch.distributed.launch --nproc_per_node=1 main.py  \
--LoRA True --epochs 120 --model SLAK_Attention_tiny --drop_path 0.1 --batch_size 128 --lr 4e-3 --update_freq 8 --model_ema true \
--model_ema_eval true --data_path /projects/2/managed_datasets/imagenet/ --num_workers 18 --kernel-size 51 49 47 13 5 --bn True


source deactivate
